# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ --------------------------------------------------------------
from time import sleep
from requests_html import HTMLSession
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

#ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ··é›‘çŠ¶æ³ã‚’å°å…¥ã—ã¦ã„ã‚‹æ–½è¨­ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸url
url = "https://onsen.nifty.com/ip-konzatsu/"
#ãƒ‹ãƒ•ãƒ†ã‚£æ¸©æ³‰ã®ãƒ¡ã‚¤ãƒ³ãƒ³ãƒšãƒ¼ã‚¸
base_url = "https://onsen.nifty.com"

#ã‚¢ã‚¯ã‚»ã‚¹çµæœã‚’ã€å¤‰æ•°rã«æ ¼ç´
r = requests.get(url, timeout=3)
#ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«2ç§’ç©ºã‘ã‚‹
sleep(2)
r.raise_for_status()
#å–å¾—çµæœã‚’è§£æã—ã¦å¤‰æ•°soupã«æ ¼ç´
soup = BeautifulSoup(r.content, "lxml")

#æ··é›‘çŠ¶æ³ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã«åŠ ç›Ÿã—ã¦ã„ã‚‹å„æ–½è¨­ã®ã‚«ãƒ¼ãƒ‰ã‚’å–å¾—
contents = soup.find_all("div", class_="facility")

#ç©ºã®ãƒªã‚¹ãƒˆã‚’ç”¨æ„
d_list = []

#å„æ¸©æ³‰æ–½è¨­ã®æƒ…å ±ã‚’å–å¾—
for i,content in enumerate(contents, start=1):
    #å–å¾—ã—ãŸã„éƒ½é“åºœçœŒã‚’æŠ½å‡º
    prefectures = content.find("span", class_="areaOnecol")
    if not prefectures.find(text=re.compile("åŸ¼ç‰çœŒ")):
        continue
    #æ–½è¨­åã‚’å–å¾—
    facility_name = content.find("div", class_="titleOnecol").find("a").text
    a_tag = content.find("a").get("href")
    #å„æ–½è¨­ã®è©³ç´°ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã®url
    page_url = base_url + a_tag + "#congestionInfo"
    #ã“ã®ãƒšãƒ¼ã‚¸ã¯javascriptã‚’ä½¿ç”¨ã—ãŸå‹•çš„ãªãƒšãƒ¼ã‚¸ã®ãŸã‚
    # requsts_htmlãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®HTMLSessionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒå¿…è¦
    s = HTMLSession()
    r = s.get(page_url,timeout=3)
    #ã“ã“ã§ãƒšãƒ¼ã‚¸ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
    # timeout=20ã®ç†ç”±ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(timeout=8)ã ã¨æ™‚é–“ãŒçŸ­ããƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«é–“ã«åˆã‚ãªã„äº‹ä¾‹ãŒå¤šã„ãŸã‚
    r.html.render(timeout=20)
    sleep(3)
    page_session = r.html
    #æ–½è¨­ã®è©•ä¾¡ã‚’å–å¾—
    evaluation = page_session.find("dl.evaluation1", first=True).find("span.score", first=True).text
    outlines = page_session.find("div.outlineInner2",first=True).find("tr", first=False)
    #æ–½è¨­ã®ä½æ‰€ã‚’å–å¾—
    address = outlines[2].find("td", first=True).text
    
    #æ–½è¨­ã®å–¶æ¥­æ™‚é–“ã‚’å–å¾—
    if outlines[4].find("th", first=True).text == "å–¶æ¥­æ™‚é–“":
        business_hour = outlines[4].find("td", first=True).text
    else:
         business_hour = "éæ²è¼‰"
    #æ–½è¨­ã®åˆ©ç”¨æ–™é‡‘ã‚’å–å¾—
    if  page_session.find("dl.dateList1", first=True):
        price = page_session.find("dl.dateList1", first=True).find("dd",first=True).text
    else:
        price = "éæ²è¼‰"
    #æ–½è¨­ã®å…¬å¼HPã‚’å–å¾—
    if outlines[-1].find("th", first=True).text == "å…¬å¼HP":
        official_hp = outlines[-1].find("td", first=True).text
    else:
        official_hp = "éæ²è¼‰"
    #æ–½è¨­ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ã‚’å–å¾—
    access = page_session.find("dl.dateList2", first=True).find("dd", first=False)[0].text

    #æ··é›‘çŠ¶æ³ã®çµµã¨çµµæ–‡å­—ã®å¯¾å¿œãƒªã‚¹ãƒˆ
    number_list = {
        "/congestion/images/crowd_icon/01_not_crowd.png": "ğŸ˜„",
        "/congestion/images/crowd_icon/02_normal.png": "ğŸ˜ƒ",
        "/congestion/images/crowd_icon/03_little_crowd.png": "ğŸ˜",
        "/congestion/images/crowd_icon/04_crowd.png": "ğŸ˜°",
        "/congestion/images/crowd_icon/05_much_crowd.png": "ğŸ¥µ",
        "/congestion/images/crowd_icon/06_close.png": "å–¶æ¥­æ™‚é–“å¤–",
    }
    
    #æ··é›‘çŠ¶æ³ãƒªã‚¹ãƒˆ
    congestion_list = []
    #æ··é›‘çŠ¶æ³ã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
    updated_date = page_session.find("p.currentState", first=True).text
    #æ··é›‘çŠ¶æ³ã®ã‚¨ãƒªã‚¢åã¨åº¦åˆã‚’å–å¾—
    congestions = page_session.find("div.mdl-card-height", first=False)
    for congestion in congestions:
        congested_area = congestion.find("h3.mdl-card__title-text", first=True).text
        try:
            number_of_people = congestion.find("img", first=True).attrs["src"]
            congestion_list.append({
                congested_area : number_list[number_of_people]
            })
        except:
            number_of_people = "éè¡¨ç¤º"
            congestion_list.append({
                congested_area : number_of_people
            })
        sleep(1)
    #pandasã§è¡¨ã«ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã€è¾æ›¸å‹ã§ãƒªã‚¹ãƒˆã«è¿½åŠ 
    d_list.append({
        "æ–½è¨­å": facility_name,
        "ä½æ‰€": address,
        "è©•ä¾¡ (5ç‚¹æº€ç‚¹)": evaluation,
        "æ··é›‘çŠ¶æ³ (ğŸ˜„ç©ºã„ã¦ã„ã‚‹ã€ğŸ˜ƒã‚„ã‚„ç©ºã„ã¦ã„ã‚‹ã€ğŸ˜æ™®é€šã€ğŸ˜°ã‚„ã‚„æ··é›‘ã€ğŸ¥µæ··é›‘)"
        : f"{updated_date} / {congestion_list}",
        "å–¶æ¥­æ™‚é–“": business_hour,
        "ä¾¡æ ¼": price,
        "ã‚¢ã‚¯ã‚»ã‚¹": access,
        "å…¬å¼HP": official_hp 
    })
    sleep(1)
    print(d_list)

print(d_list)
#pandasã‚’ä½¿ã£ã¦ã€åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ã«ã™ã‚‹
df = pd.DataFrame(d_list)
#è¡¨ã‚’csvå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
df.to_csv("saitama.csv", index=None, encoding="utf-8-sig")

