from time import sleep
from requests_html import HTMLSession
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

url = "https://onsen.nifty.com/ip-konzatsu/"
base_url = "https://onsen.nifty.com"

r = requests.get(url, timeout=3)
sleep(2)
r.raise_for_status()
soup = BeautifulSoup(r.content, "lxml")

#å…¨å›½ã®æ¸©æ³‰æ–½è¨­ã®æ¬„ã‚’å–å¾—
contents = soup.find_all("div", class_="facility")
d_list = []
#å„æ¸©æ³‰æ–½è¨­ã®æƒ…å ±ã‚’å–å¾—
for i,content in enumerate(contents, start=1):
    print("="*30, i, "="*30)
    #å–å¾—ã—ãŸã„éƒ½é“åºœçœŒã‚’æŠ½å‡º
    prefectures = content.find("span", class_="areaOnecol")
    # if not prefectures.find(text=re.compile("")):
    #     continue
    if i == 11:
        break

    facility_name = content.find("div", class_="titleOnecol").find("a").text
    a_tag = content.find("a").get("href")
    page_url = base_url + a_tag + "#congestionInfo"
    #ã“ã®ãƒšãƒ¼ã‚¸ã¯å‹•çš„ãªãŸã‚requsts_htmlãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦
    s = HTMLSession()
    r = s.get(page_url,timeout=3)
    r.html.render(timeout=20)
    print(r.status_code)
    sleep(3)
    page_soup = r.html
    evaluation = page_soup.find("dl.evaluation1", first=True).find("span.score", first=True).text
    outlines = page_soup.find("div.outlineInner2",first=True).find("tr", first=False)
    address = outlines[2].find("td", first=True).text
    
    if outlines[4].find("th", first=True).text == "å–¶æ¥­æ™‚é–“":
        business_hour = outlines[4].find("td", first=True).text
    else:
         business_hour = "éæ²è¼‰"
    if  page_soup.find("dl.dateList1", first=True):
        price = page_soup.find("dl.dateList1", first=True).text
    else:
        price = "éæ²è¼‰"

    if outlines[-1].find("th", first=True).text == "å…¬å¼HP":
        official_hp = outlines[-1].find("td", first=True).text
    else:
        official_hp = "éæ²è¼‰"

    access = page_soup.find("dl.dateList2", first=True).find("dd", first=False)[0].text

    #æ··é›‘çŠ¶æ³ã®çµµã¨äººæ•°ã®å¯¾å¿œãƒªã‚¹ãƒˆ
    number_list = {
        "/congestion/images/crowd_icon/01_not_crowd.png": "ğŸ˜„",
        "/congestion/images/crowd_icon/02_normal.png": "ğŸ˜ƒ",
        "/congestion/images/crowd_icon/03_little_crowd.png": "ğŸ˜€",
        "/congestion/images/crowd_icon/04_crowd.png": "ğŸ˜¥",
        "/congestion/images/crowd_icon/05_much_crowd.png": "ğŸ¥µ",
        "/congestion/images/crowd_icon/06_close.png": "å–¶æ¥­æ™‚é–“å¤–",
        # "/congestion/images/crowd_icon/01_not_crowd.png": "ğŸ˜„ç©ºã„ã¦ã„ã‚‹",
        # "/congestion/images/crowd_icon/02_normal.png": "ğŸ˜ƒã‚„ã‚„ç©ºã„ã¦ã„ã‚‹",
        # "/congestion/images/crowd_icon/03_little_crowd.png": "ğŸ˜€æ™®é€š",
        # "/congestion/images/crowd_icon/04_crowd.png": "ğŸ˜¥ã‚„ã‚„æ··é›‘",
        # "/congestion/images/crowd_icon/05_much_crowd.png": "ğŸ¥µæ··é›‘",
        # "/congestion/images/crowd_icon/06_close.png": "å–¶æ¥­æ™‚é–“å¤–",
    }
    
    
    #æ··é›‘çŠ¶æ³ãƒªã‚¹ãƒˆ
    congestion_list = []
    updated_date = page_soup.find("p.currentState", first=True).text
    congestions = page_soup.find("div.mdl-card-height", first=False)
    for congestion in congestions:
        area = congestion.find("h3.mdl-card__title-text", first=True).text
        try:
            number_of_people = congestion.find("img", first=True).attrs["src"]
            congestion_list.append({
                area : number_list[number_of_people]
            })
        except:
            number_of_people = "éè¡¨ç¤º"
            congestion_list.append({
                area : number_of_people
            })
        sleep(1)

    d_list.append({
        "æ–½è¨­å": facility_name,
        "ä½æ‰€": address,
        "è©•ä¾¡(5ç‚¹æº€ç‚¹)": evaluation,
        "æ··é›‘çŠ¶æ³(ğŸ˜„ç©ºã„ã¦ã„ã‚‹,ğŸ˜ƒã‚„ã‚„ç©ºã„ã¦ã„ã‚‹,ğŸ˜€æ™®é€š,ğŸ˜¥ã‚„ã‚„æ··é›‘,ğŸ¥µæ··é›‘)"
        : f"{updated_date} / {congestion_list}",
        # "æ··é›‘çŠ¶æ³": f"{updated_date} / {congestion_list}",
        "å–¶æ¥­æ™‚é–“": business_hour,
        "ä¾¡æ ¼": price,
        "ã‚¢ã‚¯ã‚»ã‚¹": access,
        "å…¬å¼HP": official_hp 
    })
    sleep(1)
    print(d_list)

print(d_list)
df = pd.DataFrame(d_list)
df.to_csv("test.csv", index=None, encoding="utf-8-sig")

